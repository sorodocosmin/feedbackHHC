On the random forest classifier, we tried an hyperparameters tunning. using GridSearch and also Random Search, here are the parameters we tried to tune:
    possible_parameters_rf = {
        "n_estimators": [10, 100, 200, 500, 750, 1_000],
        "criterion": ["gini", "entropy"],
        # the maximum depth of the tree is represented by the nr of features which is 12
        "max_depth": [None, 5, 7, 8, 9, 10],
        # Bootstrap means that instead of training on all the observations,
        # each tree of RF is trained on a subset of the observations
        "bootstrap": [True, False]
    }

And here are the results that we got :
1. Grid Search
    - Best parameters: {'bootstrap': False, 'criterion': 'entropy', 'max_depth': None, 'n_estimators': 750}
    - Best score: 0.8030440587449933 -> which represents the best score that we got on the CrossValidation(=5)
    - Time elapsed: 613.4225769042969s ~ 10.22 minutes
2. Random Search
    - Best parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 10, 'n_estimators': 750}
    - Best score: 0.7926340898976413
    - Time elapsed: 29.964808225631714s ~ 0.5 minutes

We can see that the GridSearch took much more time than the RandomSearch, and the best score is not such a big difference,
the time difference for RandomSearch is with 95% less than the time for GridSearch which considering the small diff
between the best scores, it is very good.


Now we run the algorithm of 10 different random splits of the data into training and test sets, (25% for the test), using the
default parameters for the random forest classifier, and here are the results:




------------------------------------------------------------------------------------------------------------------------
In cele ce urmeaza, se va folosi Random Forest Classifier cu parametrii default



------------------------------------------------------------------------------------------------------------------------

Hyperparameters tunning

Fitting 10 folds for each of 10 candidates, totalling 100 fits
C:\Users\sorod\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(
Best parameters: {'n_estimators': 500, 'max_depth': None, 'criterion': 'gini', 'bootstrap': True} ; Best score: 0.7612925748284864
Time elapsed for random Search: 141.7917718887329
Fitting 10 folds for each of 144 candidates, totalling 1440 fits
C:\Users\sorod\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.
  warnings.warn(
Best parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'n_estimators': 500} ; Best score: 0.7673517090644162
Time elapsed for grid Search: 2126.8367805480957



Without Hyperparameters :
Time elapsed for training the data: 0.7823429107666016
Accuracy testing: 0.740924092409241
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 0.8890128135681152
Accuracy testing: 0.7376237623762376
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 0.7317807674407959
Accuracy testing: 0.731023102310231
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 0.7592592239379883
Accuracy testing: 0.731023102310231
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 0.8641629219055176
Accuracy testing: 0.7508250825082509
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 0.7537415027618408
Accuracy testing: 0.7128712871287128
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 0.7391681671142578
Accuracy testing: 0.698019801980198
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 0.780968427658081
Accuracy testing: 0.7425742574257426
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 0.7389662265777588
Accuracy testing: 0.740924092409241
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 0.7349300384521484
Accuracy testing: 0.7293729372937293
Accuracy training: 0.9994499449944995

Average accuracy testing: 0.7315181518151814
Average accuracy training: 0.9992299229922992
Average time elapsed: 0.7774333000183106



With Hyperparameters :
Time elapsed for training the data: 3.9167683124542236
Accuracy testing: 0.7508250825082509
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 3.807605266571045
Accuracy testing: 0.7788778877887789
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 4.1581830978393555
Accuracy testing: 0.7755775577557755
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 4.663764238357544
Accuracy testing: 0.7772277227722773
Accuracy training: 1.0

Time elapsed for training the data: 4.03906512260437
Accuracy testing: 0.735973597359736
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 4.2098188400268555
Accuracy testing: 0.768976897689769
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 5.430366277694702
Accuracy testing: 0.7524752475247525
Accuracy training: 0.9988998899889989

Time elapsed for training the data: 5.679520606994629
Accuracy testing: 0.7475247524752475
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 5.582239389419556
Accuracy testing: 0.7557755775577558
Accuracy training: 0.9994499449944995

Time elapsed for training the data: 4.6425347328186035
Accuracy testing: 0.7656765676567657
Accuracy training: 0.9988998899889989

Average accuracy testing: 0.7608910891089109
Average accuracy training: 0.9991199119911991
Average time elapsed: 4.612986588478089



Cross fold validation: 100
Cross fold validation: 0.762263157894737
Time elapsed for cross fold validation: 541.2269668579102

